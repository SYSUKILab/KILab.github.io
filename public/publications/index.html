<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=light data-auto-appearance=true><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=theme-color content="#FFFFFF"><title>Recent Publications &#183; KILabðŸš€</title><meta name=title content="Recent Publications &#183; KILabðŸš€"><script type=text/javascript src=https://kilab.site/js/appearance.min.8a082f81b27f3cb2ee528df0b0bdc39787034cf2cc34d4669fbc9977c929023c.js integrity="sha256-iggvgbJ/PLLuUo3wsL3Dl4cDTPLMNNRmn7yZd8kpAjw="></script><link type=text/css rel=stylesheet href=https://kilab.site/css/main.bundle.min.0fc2ef930d53c70c53effb01f12118005a26719d9223a1599730c1d26ee3f132.css integrity="sha256-D8Lvkw1TxwxT7/sB8SEYAFomcZ2SI6FZlzDB0m7j8TI="><script defer type=text/javascript id=script-bundle src=https://kilab.site/js/main.bundle.min.0b250a079f6c2f7d0e03d1f0aa1308acb88137e3caebe1268f7478f0c87c5bf8.js integrity="sha256-CyUKB59sL30OA9HwqhMIrLiBN+PK6+Emj3R48Mh8W/g=" data-copy=Copy data-copied=Copied></script><meta name=description content="
      Publications from Knowledge Intelligence Lab
    "><link rel=canonical href=https://kilab.site/publications/><link rel=alternate type=application/rss+xml href=https://kilab.site/publications/index.xml title=KILabðŸš€><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/site.webmanifest><meta property="og:url" content="https://kilab.site/publications/"><meta property="og:site_name" content="KILabðŸš€"><meta property="og:title" content="Recent Publications"><meta property="og:description" content="Publications from Knowledge Intelligence Lab"><meta property="og:locale" content="en"><meta property="og:type" content="website"><meta name=twitter:card content="summary"><meta name=twitter:title content="Recent Publications"><meta name=twitter:description content="Publications from Knowledge Intelligence Lab"><meta name=author content="KILab"></head><body class="m-auto flex h-screen max-w-7xl flex-col bg-neutral px-6 text-lg leading-7 text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32"><div id=the-top class="absolute flex self-center"><a class="-translate-y-8 rounded-b-lg bg-primary-200 px-3 py-1 text-sm focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="pe-2 font-bold text-primary-600 dark:text-primary-400">&darr;</span>Skip to main content</a></div><header class="py-6 font-semibold text-neutral-900 dark:text-neutral sm:py-10 print:hidden"><nav class="flex items-start justify-between sm:items-center"><div class="flex flex-row items-center"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2" rel=me href=/>KILabðŸš€</a></div><ul class="flex list-none flex-col text-end sm:flex-row"><li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5"><a href=/posts/ title><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2">News</span></a></li><li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5"><a href=/publications/ title="Recent Publications"><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2">Publications</span></a></li><li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5"><a href=/team/ title="Team Members"><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2">Team</span></a></li><li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5"><button id=search-button-1 title="Search (/)">
<span class="group-dark:hover:text-primary-400 transition-colors group-hover:text-primary-600"><span class="icon relative inline-block px-1 align-text-bottom"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span></span><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2"></span></button></li></ul></nav></header><div class="relative flex grow flex-col"><main id=main-content class=grow><article class=max-w-full><header><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">Recent Publications</h1></header><section class="prose mt-6 max-w-full dark:prose-invert"><p>Professor Lyu has been engaged in research in intelligent information retrieval, natural language processing, and trustworthy artificial intelligence for many years, publishing over 30 academic papers in CCF A/B journals and conferences in recent years.</p><p>The following are representative research achievements from our laboratory:</p><h3 id=2025 class="relative group">2025 <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line:none !important" href=#2025 aria-label=Anchor>#</a></span></h3><h4 id=hahahugoshortcode17s0hbhb-dani-discrepancy-assessing-for-natural-and-ai-images class="relative group"><span class="icon relative inline-block align-text-bottom"><svg viewBox="0 0 512 512"><path fill="currentColor" d="M421.7 220.3 188.5 453.4l-33.9-33.9 3.5-3.5H112C103.2 416 96 408.8 96 4e2V353.9l-3.49 3.5C87.78 362.2 84.31 368 82.42 374.4L59.44 452.6l78.16-23C143.1 427.7 149.8 424.2 154.6 419.5l33.9 33.9c-10.4 10.4-23.3 18.1-37.4 22.2L30.77 511C22.35 513.5 13.24 511.2 7.03 504.1.8198 498.8-1.502 489.7.976 481.2L36.37 360.9c4.16-14.1 11.79-27 22.2-37.4L291.7 90.34l130 129.96zm71-161.55C517.7 83.74 517.7 124.3 492.7 149.3l-48.4 48.4-130-129.98 48.4-48.4c25-24.998 65.6-24.998 90.6.0l39.4 39.43z"/></svg>
</span>DANI: Discrepancy Assessing for Natural and AI Images <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line:none !important" href=#hahahugoshortcode17s0hbhb-dani-discrepancy-assessing-for-natural-and-ai-images aria-label=Anchor>#</a></span></h4><p><strong>Authors</strong>: Renyang Liu, <strong>Ziyu Lyu</strong>, Wei Zhou, See-Kiong Ng<br><strong>Type</strong>: Dataset<br><strong>Associated Paper</strong>: D-Judge: How Far Are We? Assessing the Discrepancies Between AI-synthesized Images and Natural Images through Multimodal Guidance (ACM MM 2025)</p><p><a class="inline-block !rounded-md bg-primary-600 px-4 py-1 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://huggingface.co/datasets/Renyang/DANI role=button>ðŸ¤— Dataset
</a><a class="inline-block !rounded-md bg-primary-600 px-4 py-1 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://github.com/ryliu68/DJudge role=button>Code
</a><a class="inline-block !rounded-md bg-primary-600 px-4 py-1 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=/papers/2412.17632v2.pdf role=button>Paper PDF</a></p><h4 id=hahahugoshortcode17s4hbhb-d-judge-how-far-are-we-assessing-the-discrepancies-between-ai-synthesized-images-and-natural-images-through-multimodal-guidance class="relative group"><span class="icon relative inline-block align-text-bottom"><svg viewBox="0 0 512 512"><path fill="currentColor" d="M421.7 220.3 188.5 453.4l-33.9-33.9 3.5-3.5H112C103.2 416 96 408.8 96 4e2V353.9l-3.49 3.5C87.78 362.2 84.31 368 82.42 374.4L59.44 452.6l78.16-23C143.1 427.7 149.8 424.2 154.6 419.5l33.9 33.9c-10.4 10.4-23.3 18.1-37.4 22.2L30.77 511C22.35 513.5 13.24 511.2 7.03 504.1.8198 498.8-1.502 489.7.976 481.2L36.37 360.9c4.16-14.1 11.79-27 22.2-37.4L291.7 90.34l130 129.96zm71-161.55C517.7 83.74 517.7 124.3 492.7 149.3l-48.4 48.4-130-129.98 48.4-48.4c25-24.998 65.6-24.998 90.6.0l39.4 39.43z"/></svg>
</span>D-Judge: How Far Are We? Assessing the Discrepancies Between AI-synthesized Images and Natural Images through Multimodal Guidance <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line:none !important" href=#hahahugoshortcode17s4hbhb-d-judge-how-far-are-we-assessing-the-discrepancies-between-ai-synthesized-images-and-natural-images-through-multimodal-guidance aria-label=Anchor>#</a></span></h4><p><strong>Authors</strong>: Renyang Liu, <strong>Ziyu Lyu</strong>, Wei Zhou, See-Kiong Ng<br><strong>Conference</strong>: ACM International Conference on Multimedia (MM), 2025<br><strong>Rank</strong>: CCF-A</p><p><a class="inline-block !rounded-md bg-primary-600 px-4 py-1 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=/papers/2412.17632v2.pdf role=button>PDF
</a><a class="inline-block !rounded-md bg-primary-600 px-4 py-1 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://arxiv.org/abs/2412.17632 role=button>arXiv
</a><a class="inline-block !rounded-md bg-primary-600 px-4 py-1 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://github.com/ryliu68/DJudge role=button>Code
</a><a class="inline-block !rounded-md bg-primary-600 px-4 py-1 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://huggingface.co/datasets/Renyang/DANI role=button>ðŸ¤— Dataset</a></p><h3 id=2024 class="relative group">2024 <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line:none !important" href=#2024 aria-label=Anchor>#</a></span></h3><h3 id=2023 class="relative group">2023 <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line:none !important" href=#2023 aria-label=Anchor>#</a></span></h3><h4 id=hahahugoshortcode17s9hbhb-you-augment-me-exploring-chatgpt-based-data-augmentation-for-semantic-code-search class="relative group"><span class="icon relative inline-block align-text-bottom"><svg viewBox="0 0 512 512"><path fill="currentColor" d="M421.7 220.3 188.5 453.4l-33.9-33.9 3.5-3.5H112C103.2 416 96 408.8 96 4e2V353.9l-3.49 3.5C87.78 362.2 84.31 368 82.42 374.4L59.44 452.6l78.16-23C143.1 427.7 149.8 424.2 154.6 419.5l33.9 33.9c-10.4 10.4-23.3 18.1-37.4 22.2L30.77 511C22.35 513.5 13.24 511.2 7.03 504.1.8198 498.8-1.502 489.7.976 481.2L36.37 360.9c4.16-14.1 11.79-27 22.2-37.4L291.7 90.34l130 129.96zm71-161.55C517.7 83.74 517.7 124.3 492.7 149.3l-48.4 48.4-130-129.98 48.4-48.4c25-24.998 65.6-24.998 90.6.0l39.4 39.43z"/></svg>
</span>You Augment Me: Exploring ChatGPT-based Data Augmentation for Semantic Code Search <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line:none !important" href=#hahahugoshortcode17s9hbhb-you-augment-me-exploring-chatgpt-based-data-augmentation-for-semantic-code-search aria-label=Anchor>#</a></span></h4><p><strong>Authors</strong>: Yanlin Wang, Lianghong Guo, Ensheng Shi, Wenqing Chen, Jiachi Chen, Wanjun Zhong, Menghan Wang, Hui Li, <strong>Ziyu Lyu</strong>, Hongyu Zhang, and Zibin Zheng<br><strong>Conference</strong>: IEEE International Conference on Software Maintenance and Evolution (ICSME), 2023<br><strong>Rank</strong>: CCF-B</p><a class="inline-block !rounded-md bg-primary-600 px-4 py-1 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=/papers/2408.05542v2.pdf role=button>PDF
</a><a class="inline-block !rounded-md bg-primary-600 px-4 py-1 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://anonymous.4open.science/r/ChatDANCE role=button>Code & Data</a><h4 id=hahahugoshortcode17s12hbhb-information-retrieval-meets-large-language-models-a-strategic-report-from-chinese-ir-community class="relative group"><span class="icon relative inline-block align-text-bottom"><svg viewBox="0 0 512 512"><path fill="currentColor" d="M421.7 220.3 188.5 453.4l-33.9-33.9 3.5-3.5H112C103.2 416 96 408.8 96 4e2V353.9l-3.49 3.5C87.78 362.2 84.31 368 82.42 374.4L59.44 452.6l78.16-23C143.1 427.7 149.8 424.2 154.6 419.5l33.9 33.9c-10.4 10.4-23.3 18.1-37.4 22.2L30.77 511C22.35 513.5 13.24 511.2 7.03 504.1.8198 498.8-1.502 489.7.976 481.2L36.37 360.9c4.16-14.1 11.79-27 22.2-37.4L291.7 90.34l130 129.96zm71-161.55C517.7 83.74 517.7 124.3 492.7 149.3l-48.4 48.4-130-129.98 48.4-48.4c25-24.998 65.6-24.998 90.6.0l39.4 39.43z"/></svg>
</span>Information Retrieval Meets Large Language Models: A Strategic Report from Chinese IR Community <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line:none !important" href=#hahahugoshortcode17s12hbhb-information-retrieval-meets-large-language-models-a-strategic-report-from-chinese-ir-community aria-label=Anchor>#</a></span></h4><p><strong>Authors</strong>: Qingyao Ai, Ting Bai, Zhao Cao, Yi Chang, Jiawei Chen, Zhumin Chen, Zhiyong Cheng, Shoubin Dong, Zhicheng Dou, Fuli Feng, Shen Gao, Jiafeng Guo, Xiangnan He, Yanyan Lan, Chenliang Li, Yiqun Liu, <strong>Ziyu Lyu</strong>, Weizhi Ma, Jun Ma, Zhaochun Ren, Pengjie Ren, Zhiqiang Wang, Mingwen Wang, Ji-Rong Wen, Le Wu, Xin Xin, Jun Xu, Dawei Yin, Peng Zhang, Fan Zhang, Weinan Zhang, Min Zhang, Xiaofei Zhu<br><strong>Journal</strong>: AI Open, 2023</p><a class="inline-block !rounded-md bg-primary-600 px-4 py-1 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://arxiv.org/abs/2307.09751 role=button>arXiv</a><h3 id=2022 class="relative group">2022 <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line:none !important" href=#2022 aria-label=Anchor>#</a></span></h3><h4 id=hahahugoshortcode17s14hbhb-knowledge-enhanced-graph-neural-networks-for-explainable-recommendation class="relative group"><span class="icon relative inline-block align-text-bottom"><svg viewBox="0 0 512 512"><path fill="currentColor" d="M421.7 220.3 188.5 453.4l-33.9-33.9 3.5-3.5H112C103.2 416 96 408.8 96 4e2V353.9l-3.49 3.5C87.78 362.2 84.31 368 82.42 374.4L59.44 452.6l78.16-23C143.1 427.7 149.8 424.2 154.6 419.5l33.9 33.9c-10.4 10.4-23.3 18.1-37.4 22.2L30.77 511C22.35 513.5 13.24 511.2 7.03 504.1.8198 498.8-1.502 489.7.976 481.2L36.37 360.9c4.16-14.1 11.79-27 22.2-37.4L291.7 90.34l130 129.96zm71-161.55C517.7 83.74 517.7 124.3 492.7 149.3l-48.4 48.4-130-129.98 48.4-48.4c25-24.998 65.6-24.998 90.6.0l39.4 39.43z"/></svg>
</span>Knowledge Enhanced Graph Neural Networks for Explainable Recommendation <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line:none !important" href=#hahahugoshortcode17s14hbhb-knowledge-enhanced-graph-neural-networks-for-explainable-recommendation aria-label=Anchor>#</a></span></h4><p><strong>Authors</strong>: <strong>Ziyu Lyu</strong>, Yue Wu, Junjie Lai, Min Yang, Chengming Li, Wei Zhou<br><strong>Journal</strong>: IEEE Transactions on Knowledge and Data Engineering (TKDE), 2022<br><strong>Rank</strong>: CCF-A / JCR Q1</p><a class="inline-block !rounded-md bg-primary-600 px-4 py-1 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://ieeexplore.ieee.org/document/9681226 role=button>IEEE Xplore</a><h4 id=hahahugoshortcode17s16hbhb-visual-knowledge-graph-for-human-action-reasoning-in-videos class="relative group"><span class="icon relative inline-block align-text-bottom"><svg viewBox="0 0 512 512"><path fill="currentColor" d="M421.7 220.3 188.5 453.4l-33.9-33.9 3.5-3.5H112C103.2 416 96 408.8 96 4e2V353.9l-3.49 3.5C87.78 362.2 84.31 368 82.42 374.4L59.44 452.6l78.16-23C143.1 427.7 149.8 424.2 154.6 419.5l33.9 33.9c-10.4 10.4-23.3 18.1-37.4 22.2L30.77 511C22.35 513.5 13.24 511.2 7.03 504.1.8198 498.8-1.502 489.7.976 481.2L36.37 360.9c4.16-14.1 11.79-27 22.2-37.4L291.7 90.34l130 129.96zm71-161.55C517.7 83.74 517.7 124.3 492.7 149.3l-48.4 48.4-130-129.98 48.4-48.4c25-24.998 65.6-24.998 90.6.0l39.4 39.43z"/></svg>
</span>Visual Knowledge Graph for Human Action Reasoning in Videos <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line:none !important" href=#hahahugoshortcode17s16hbhb-visual-knowledge-graph-for-human-action-reasoning-in-videos aria-label=Anchor>#</a></span></h4><p><strong>Authors</strong>: Yue Ma, Yali Wang, Yue Wu, <strong>Ziyu Lyu</strong>, Siran Chen, Xiu Li, Yu Qiao<br><strong>Conference</strong>: Proceedings of the 29th ACM International Conference on Multimedia (MM), 2022<br><strong>Rank</strong>: CCF-A</p><a class="inline-block !rounded-md bg-primary-600 px-4 py-1 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=/papers/visual_kg.pdf role=button>PDF</a><hr><h2 id=complete-publication-statistics class="relative group">Complete Publication Statistics <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line:none !important" href=#complete-publication-statistics aria-label=Anchor>#</a></span></h2><h3 id=by-category class="relative group">By Category <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line:none !important" href=#by-category aria-label=Anchor>#</a></span></h3><p><strong>Journal Papers</strong>:</p><ul><li><strong>CCF-A / JCR Q1</strong>: TKDE (2 papers)</li><li><strong>CCF-B / JCR Q1</strong>: Information Sciences (2 papers), Neural Networks (1 paper)</li><li><strong>Other High-Quality Journals</strong>: 5 papers</li></ul><p><strong>Conference Papers</strong>:</p><ul><li><strong>CCF-A</strong>: AAAI (1 paper), SIGIR (1 paper), MM (1 paper)</li><li><strong>CCF-B</strong>: ICSME (1 paper), ICWSM (1 paper), SDM (1 paper)</li><li><strong>Other Important Conferences</strong>: 8 papers</li></ul><hr><div class="flex rounded-md bg-primary-100 px-4 py-3 dark:bg-primary-900"><span class="pe-3 text-primary-400"><span class="icon relative inline-block px-1 align-text-bottom"><svg viewBox="0 0 640 512"><path fill="currentColor" d="M172.5 131.1c55.6-55.59 148-55.59 203.6.0 50 50 57.4 129.7 16.3 187.2L391.3 319.9C381 334.2 361 337.6 346.7 327.3c-14.4-10.3-17.8-30.3-7.5-44.6L340.3 281.1C363.2 249 359.6 205.1 331.7 177.2c-31.4-31.4-82.5-31.4-114 0L105.5 289.5c-31.51 30.6-31.51 82.5.0 114C133.3 431.4 177.3 435 209.3 412.1L210.9 410.1C225.3 400.7 245.3 404 255.5 418.4 265.8 432.8 262.5 452.8 248.1 463.1L246.5 464.2c-58.4 41.1-136.3 34.5-186.29-15.4-56.469-56.5-56.469-148.1.0-204.5L172.5 131.1zM467.5 380c-56.5 56.5-148 56.5-204.5.0-50-50-56.5-128.8-15.4-186.3L248.7 192.1C258.1 177.8 278.1 174.4 293.3 184.7 307.7 194.1 311.1 214.1 300.8 229.3L299.7 230.9C276.8 262.1 280.4 306.9 308.3 334.8c31.4 31.4 82.5 31.4 114 0L534.5 222.5c31.5-31.5 31.5-83.4.0-114C506.7 80.63 462.7 76.99 430.7 99.9L429.1 101C414.7 111.3 394.7 107.1 384.5 93.58 374.2 79.2 377.5 59.21 391.9 48.94L393.5 47.82C451 6.731 529.8 13.25 579.8 63.24c56.5 56.46 56.5 148.06.0 204.46L467.5 380z"/></svg>
</span></span><span class=dark:text-neutral-300><strong>More Publications</strong>: For the complete list of publications, please visit
<a href=https://kilab.site/full_paperlist/>Full_Paperlist</a> or <a href=https://dblp.uni-trier.de/pers/hd/l/Lu:Ziyu.html target=_blank rel=noreferrer>DBLP</a>.</span></div><div class="flex rounded-md bg-primary-100 px-4 py-3 dark:bg-primary-900"><span class="pe-3 text-primary-400"><span class="icon relative inline-block px-1 align-text-bottom"><svg viewBox="0 0 384 512"><path fill="currentColor" d="M112.1 454.3c0 6.297 1.816 12.44 5.284 17.69l17.14 25.69c5.25 7.875 17.17 14.28 26.64 14.28h61.67c9.438.0 21.36-6.401 26.61-14.28l17.08-25.68c2.938-4.438 5.348-12.37 5.348-17.7L272 415.1H112L112.1 454.3zM191.4.0132C89.44.3257 16 82.97 16 175.1c0 44.38 16.44 84.84 43.56 115.8 16.53 18.84 42.34 58.23 52.22 91.45.0313.25.0938.5166.125.7823h160.2c.0313-.2656.0938-.5166.125-.7823 9.875-33.22 35.69-72.61 52.22-91.45C351.6 260.8 368 220.4 368 175.1 368 78.61 288.9-.2837 191.4.0132zM192 96.01c-44.13.0-80 35.89-80 79.1.0 9.69-7.2 16.89-16 16.89S80 184.8 80 176c0-61.76 50.25-111.1 112-111.1 8.844.0 16 7.159 16 16S200.8 96.01 192 96.01z"/></svg>
</span></span><span class=dark:text-neutral-300><strong>Academic Impact</strong>: Our papers have been cited hundreds of times, making significant impact in intelligent information retrieval, recommendation systems, and related fields</span></div></section><footer class=pt-8></footer></article></main><div class="pointer-events-none absolute bottom-0 end-0 top-[100vh] w-12" id=to-top hidden=true><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 backdrop-blur hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div><footer class="py-10 print:hidden"><nav class="pb-4 text-base font-medium text-neutral-500 dark:text-neutral-400"><ul class="flex list-none flex-col sm:flex-row"><li class="group mb-1 text-end sm:mb-0 sm:me-7 sm:last:me-0"><a href=/activities/ title="Lab Activities"><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2">Lab Activities</span></a></li><li class="group mb-1 text-end sm:mb-0 sm:me-7 sm:last:me-0"><a href=/conference_deadlines/ title="Conference Deadlines"><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2">Conference Deadlines</span></a></li></ul></nav><div class="flex items-center justify-between"><div><p class="text-sm text-neutral-500 dark:text-neutral-400">Â© 2025 Knowledge Intelligince Lab @ SYSU.</p><p class="text-xs text-neutral-500 dark:text-neutral-400">Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://github.com/jpanther/congo target=_blank rel="noopener noreferrer">Congo</a></p></div><div class="flex flex-row items-center"></div></div></footer><div id=search-wrapper class="invisible fixed inset-0 z-50 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]" data-url=https://kilab.site/><div id=search-modal class="top-20 mx-auto flex min-h-0 w-full max-w-3xl flex-col rounded-md border border-neutral-200 bg-neutral shadow-lg dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex flex-none items-center justify-between px-2"><form class="flex min-w-0 flex-auto items-center"><div class="flex h-8 w-8 items-center justify-center text-neutral-400"><span class="icon relative inline-block px-1 align-text-bottom"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="mx-1 flex h-12 flex-auto appearance-none bg-transparent focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=Search tabindex=0></form><button id=close-search-button class="flex h-8 w-8 items-center justify-center text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="Close (Esc)">
<span class="icon relative inline-block px-1 align-text-bottom"><svg viewBox="0 0 320 512"><path fill="currentColor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto overflow-auto px-2"><ul id=search-results></ul></section></div></div></div></body></html>